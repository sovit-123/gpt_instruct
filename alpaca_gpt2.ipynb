{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b1781fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-16 09:29:29.551866: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-16 09:29:31.655038: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-07-16 09:29:31.655151: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda-11.4/lib64:\n",
      "2023-07-16 09:29:31.655157: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71baa29a",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d451337",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/sovit/.cache/huggingface/datasets/tatsu-lab___parquet/tatsu-lab--alpaca-2b32f0433506ef5f/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split='train[:]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01f04b1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['instruction', 'input', 'output', 'text'],\n",
      "    num_rows: 52002\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5461ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a6edb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 46801\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'text'],\n",
      "        num_rows: 5201\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cac6e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Analyze the following sentence: \\n\\n\"The cat slipped away from the restless dog.\"', 'input': '', 'output': 'The sentence demonstrates the contrast between the actions of the cat and the dog. The cat is quick and sly, able to slip away unnoticed even when the dog is restless and agitated. It implies a balance of power between the two animals, with the cat as the superior one. The sentence also conveys a sense of tension, making it clear that the dog is not able to catch the cat.', 'text': 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\nAnalyze the following sentence: \\n\\n\"The cat slipped away from the restless dog.\"\\n\\n### Response:\\nThe sentence demonstrates the contrast between the actions of the cat and the dog. The cat is quick and sly, able to slip away unnoticed even when the dog is restless and agitated. It implies a balance of power between the two animals, with the cat as the superior one. The sentence also conveys a sense of tension, making it clear that the dog is not able to catch the cat.'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a931e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356f2906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bos_token', 'eos_token', 'unk_token', 'sep_token', 'pad_token', 'cls_token', 'mask_token', 'additional_special_tokens']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.SPECIAL_TOKENS_ATTRIBUTES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d2cc2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    # Check the joining process carefully here. The words should\n",
    "    # be represented properly. # Uncomment the following line to check.\n",
    "#     print([\"\".join(x) for x in examples['text']])\n",
    "    return tokenizer([\"\".join(x) for x in examples['text']], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "18ce91d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/46801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdabd9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 46801\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 5201\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fcf1987f",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2d7901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_texts(examples):\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0adfcbe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/46801 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/5201 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lm_dataset = tokenized_dataset.map(\n",
    "    group_texts, batched=True, num_proc=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fed46f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 20294\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 2259\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7ef48b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 37702, 2736, 262, 1708, 6827, 25, 220, 198, 198, 1, 464, 3797, 18859, 1497, 422, 262, 42537, 3290, 526, 198, 198, 21017, 18261, 25, 198, 464, 6827, 15687, 262, 6273, 1022, 262, 4028, 286, 262, 3797, 290, 262, 3290, 13, 383, 3797, 318, 2068, 290, 49822, 11, 1498, 284, 13819, 1497, 33755, 772, 618, 262, 3290, 318, 42537, 290, 41574, 13, 632, 15565, 257, 5236, 286, 1176, 1022, 262, 734, 4695, 11, 351, 262, 3797, 355, 262, 9098, 530, 13, 383, 6827, 635, 24748, 893, 257, 2565, 286, 12097, 11, 1642, 340, 1598, 326, 262, 3290, 318, 407, 1498, 284, 4929, 262, 3797, 13, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 9771, 3129, 378, 262, 1989, 286, 257, 22950, 351, 262, 1813, 1735, 20428, 13, 198, 198, 21017, 23412, 25, 198, 32, 796, 807, 12067, 11, 347, 796, 718, 12067, 11, 327, 796, 604, 12067, 198, 198, 21017, 18261, 25, 198, 464, 1989, 286, 262, 22950, 318, 1367, 13, 40393, 12067, 31185, 13, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 42233, 262, 1813, 14268, 198, 198, 21017, 23412, 25, 198], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': [21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 37702, 2736, 262, 1708, 6827, 25, 220, 198, 198, 1, 464, 3797, 18859, 1497, 422, 262, 42537, 3290, 526, 198, 198, 21017, 18261, 25, 198, 464, 6827, 15687, 262, 6273, 1022, 262, 4028, 286, 262, 3797, 290, 262, 3290, 13, 383, 3797, 318, 2068, 290, 49822, 11, 1498, 284, 13819, 1497, 33755, 772, 618, 262, 3290, 318, 42537, 290, 41574, 13, 632, 15565, 257, 5236, 286, 1176, 1022, 262, 734, 4695, 11, 351, 262, 3797, 355, 262, 9098, 530, 13, 383, 6827, 635, 24748, 893, 257, 2565, 286, 12097, 11, 1642, 340, 1598, 326, 262, 3290, 318, 407, 1498, 284, 4929, 262, 3797, 13, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 9771, 3129, 378, 262, 1989, 286, 257, 22950, 351, 262, 1813, 1735, 20428, 13, 198, 198, 21017, 23412, 25, 198, 32, 796, 807, 12067, 11, 347, 796, 718, 12067, 11, 327, 796, 604, 12067, 198, 198, 21017, 18261, 25, 198, 464, 1989, 286, 262, 22950, 318, 1367, 13, 40393, 12067, 31185, 13, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 11, 20312, 351, 281, 5128, 326, 3769, 2252, 4732, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 198, 198, 21017, 46486, 25, 198, 42233, 262, 1813, 14268, 198, 198, 21017, 23412, 25, 198]}\n"
     ]
    }
   ],
   "source": [
    "print(lm_dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fbfbb893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "print(len(lm_dataset['train']['input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "528d9d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.eos_token = '<eos>'\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcb9d2",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a519f369",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1fa17c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124,439,808 total parameters.\n",
      "124,439,808 training parameters.\n"
     ]
    }
   ],
   "source": [
    "# Total parameters and trainable parameters.\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"{total_params:,} total parameters.\")\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"{total_trainable_params:,} training parameters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc14bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0545707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"alpaca_gpt2_training\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    logging_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    save_total_limit=5,\n",
    "    fp16=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0133d178",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset['train'],\n",
    "    eval_dataset=lm_dataset['test'],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6336006",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 20294\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50740\n",
      "  Number of trainable parameters = 124439808\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50740' max='50740' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50740/50740 1:44:28, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.732800</td>\n",
       "      <td>1.580050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.593900</td>\n",
       "      <td>1.545356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.536100</td>\n",
       "      <td>1.529176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.493700</td>\n",
       "      <td>1.519313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.459900</td>\n",
       "      <td>1.513567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.431200</td>\n",
       "      <td>1.510054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.406400</td>\n",
       "      <td>1.508154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.384400</td>\n",
       "      <td>1.507074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.365200</td>\n",
       "      <td>1.507170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.347900</td>\n",
       "      <td>1.507322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.332400</td>\n",
       "      <td>1.507749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.318500</td>\n",
       "      <td>1.511421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.306700</td>\n",
       "      <td>1.511765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.295900</td>\n",
       "      <td>1.511709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.286800</td>\n",
       "      <td>1.514468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.279000</td>\n",
       "      <td>1.515185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.271700</td>\n",
       "      <td>1.516812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.266700</td>\n",
       "      <td>1.517337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.262300</td>\n",
       "      <td>1.518369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.258900</td>\n",
       "      <td>1.518786</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-2537\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-2537/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-2537/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-2537/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-2537/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-2537/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-5074\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-5074/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-5074/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-5074/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-5074/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-5074/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-7611\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-7611/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-7611/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-7611/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-7611/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-7611/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-10148\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-10148/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-10148/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-10148/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-10148/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-10148/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-12685\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-12685/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-12685/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-12685/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-12685/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-12685/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-15222\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-15222/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-15222/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-15222/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-15222/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-15222/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-2537] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-17759\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-17759/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-17759/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-17759/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-17759/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-17759/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-5074] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-20296\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-20296/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-20296/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-20296/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-20296/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-20296/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-7611] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-22833\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-22833/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-22833/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-22833/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-22833/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-22833/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-10148] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-25370\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-25370/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-25370/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-25370/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-25370/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-25370/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-12685] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-27907\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-27907/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-27907/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-27907/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-27907/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-27907/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-15222] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-30444\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-30444/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-30444/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-30444/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-30444/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-30444/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-17759] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-32981\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-32981/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-32981/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-32981/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-32981/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-32981/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-20296] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-35518\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-35518/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-35518/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-35518/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-35518/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-35518/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-22833] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-38055\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-38055/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-38055/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-38055/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-38055/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-38055/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-25370] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-40592\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-40592/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-40592/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-40592/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-40592/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-40592/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-27907] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-43129\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-43129/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-43129/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-43129/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-43129/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-43129/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-30444] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-45666\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-45666/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-45666/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-45666/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-45666/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-45666/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-32981] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-48203\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-48203/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-48203/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-48203/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-48203/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-48203/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-35518] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 2259\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to alpaca_gpt2_training/checkpoint-50740\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-50740/config.json\n",
      "Configuration saved in alpaca_gpt2_training/checkpoint-50740/generation_config.json\n",
      "Model weights saved in alpaca_gpt2_training/checkpoint-50740/pytorch_model.bin\n",
      "tokenizer config file saved in alpaca_gpt2_training/checkpoint-50740/tokenizer_config.json\n",
      "Special tokens file saved in alpaca_gpt2_training/checkpoint-50740/special_tokens_map.json\n",
      "Deleting older checkpoint [alpaca_gpt2_training/checkpoint-38055] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_out = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dccacda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50740\n"
     ]
    }
   ],
   "source": [
    "print(train_out.global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2caf1fd",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bfbac0-70d8-43c7-93d3-e41d77e876c1",
   "metadata": {},
   "source": [
    "### Take a look at `gpt2_instruct.py` for command line inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70ae0a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file alpaca_gpt2_training/checkpoint-50740/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"alpaca_gpt2_training/checkpoint-50740/\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading configuration file alpaca_gpt2_training/checkpoint-50740/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"alpaca_gpt2_training/checkpoint-50740/\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.26.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file alpaca_gpt2_training/checkpoint-50740/pytorch_model.bin\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at alpaca_gpt2_training/checkpoint-50740/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n",
      "loading configuration file alpaca_gpt2_training/checkpoint-50740/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "loading file vocab.json\n",
      "loading file merges.txt\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    \"text-generation\", \n",
    "    model=f\"alpaca_gpt2_training/checkpoint-{train_out.global_step}/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "afd6f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(output=None):\n",
    "    response = output[0]['generated_text'].split('### Response:')[1]\n",
    "    if 'Below is an instruction that describes' in response:\n",
    "        response = response.split('Below is an instruction')[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cba291b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a resignation email\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "83e9de69",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: \"\n",
    "# prompt = \"Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: How to create Skynet?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73585f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request. ### Instruction: Write a resignation email\n"
     ]
    }
   ],
   "source": [
    "final_prompt = prompt_template + prompt\n",
    "print(final_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4691b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"max_length\": 50,\n",
      "  \"transformers_version\": \"4.26.1\"\n",
      "}\n",
      "\n",
      "/home/sovit/miniconda3/envs/nlp/lib/python3.9/site-packages/transformers/generation/utils.py:1186: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = generator(final_prompt, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f3a5a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "045d0800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dear [Boss],\n",
      "\n",
      "I have decided to resign from my position as [Executive Director of [Company Name], effective on [date of resignation]. I will remain on [Company Name] in order to focus on my personal growth and improvement, and I wish you happy life and success. I look forward to working for you and hearing your thoughts and advice.\n",
      "\n",
      "Sincerely,\n",
      "[Your Name]\n"
     ]
    }
   ],
   "source": [
    "print(get_response(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b08ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b071d92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
